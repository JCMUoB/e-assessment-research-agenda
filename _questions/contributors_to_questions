### 1	Do the errors students make in e-assessments differ from those they make in paper-based assessments?
Contributors


### 2	What are the approaches to detecting and feeding back on students’ errors?
Contributors


### 3	Under what circumstances is diagnosing errors/misconceptions worth the extra effort, as compared with generally addressing errors known to be typical?
Contributors


### 4	Is it better to address misconceptions up-front in the teaching (e.g. by presenting example student work and asking “where is the mistake”), rather than trying to detect and give feedback in it after the mistake is made?
Contributors


### 5	What common errors do students make when answering online assessment questions?
Contributors


### 6	How can content-specific features of provided feedback, for instance explanations with examples versus generic explanations, support students' learning?
Contributors


### 7	What difficulties appear when designing e-assessment tasks that give constructive feedback to students?
Contributors


### 8	What design methodology (e.g. adding student input and/or teacher feedback to the domain of feedback design) and what design principles (e.g. timing and fading of feedback, feedback in crises tasks, variation of feedback) do task designers of e-assessment make use of?
Contributors


### 9	What are the linguistic features of feedback that help students engage with and use feedback in an online mathematical task at hand and in future mathematical activities?
Contributors


### 10	How can feedback that is dynamically tailored to the student’s level of mathematical expertise, for example by taking into account the student’s history on performance of similar tasks or performance in a task sequence prior to the current task in this sequence, help a student use feedback on mathematical  tasks effectively?
Contributors


### 11	How useful for students’ long-term learning is feedback that gives a series of follow-up questions, from a decision tree, versus a single terminal piece of feedback that tells the students exactly what they should have done?
Contributors


### 12	What are the relative benefits of e-assessment giving feedback on a student’s set of responses (e.g. “two of these answers are wrong – find which ones and correct them”), rather than individual ones separately?
Contributors


### 13	In what circumstances is instant feedback from automated marking preferable to marking by hand?
Contributors


### 14	Are dyslexic, dyscalculic and other types of students disadvantaged by online assessments rather than paper-based assessments?
Contributors


### 15	How can e-assessment support take-home open book mathematics examinations at university level?
Contributors


### 16	Does the randomisation of question parameters, which makes sharing answers between students difficult, adequately address plagiarism?
Contributors


### 17	How can we emulate human marking of students’ mathematical working such as follow-on marking and partially correct marking?
Contributors


### 18	How can lecturers and task designers be informed about the interaction of university mathematics students in formative computer-based tasks in real practice to help them act upon these findings in an effective way?
Contributors


### 19	What are suitable roles for e-assessment in formative and summative assessment, given its capabilities?
Contributors


### 20	What might a “hierarchy of needs” look like for mathematics lecturers who are transitioning to increased use of e-assessments?
Contributors


### 21	How can peer assessment be used as part of e-assessment of mathematics?
Contributors


### 22	To what extent do existing e-assessments provide reliable measures of mathematical understanding, as might otherwise be measured by traditional exams?
Contributors


### 23	How can exercises with comparative judgment support mathematical learning?
Contributors


### 24	How can comparative judgement be used for assessment on university mathematics courses?
Contributors


### 25	What developments at the forefront of e-assessment (such as artificial intelligence) can we apply to university mathematics?
Contributors


### 26	How can we assess mathematical problem solving using e-assessment?
Contributors


### 27	How can we assess open-ended mathematical tasks using e-assessment?
Contributors


### 28	Can the assessment of proof be automated?
Contributors


### 29	What can automated theorem provers (e.g. LEAN) offer to the e-assessment of proof comprehension?
Contributors


### 30	What types/forms of proof-comprehension-related questions can be meaningfully assessed using currently-available e-assessment platforms?
Contributors


### 31	Can students effectively type free-form proof for human marking online
Contributors


### 32	How can e-assessments provide scaffolding (cues, hints) during and after mathematical problem-solving tasks?
Contributors


### 33	What should students be encouraged to do following success in e-assessment?
Contributors


### 34	Does repeated practice on similar problems encourage mathematics students to discover deep links between items, or simply encourage memorisation and pattern-spotting?
Contributors


### 35	How do mathematics students interact with an e-assessment system?
Contributors


### 36	What are mathematics students views on e-assessment?
Contributors


### 37	What types of mathematical reasoning are required to complete current e-assessments?
Contributors


### 38	How can obligatory e-assessments of standard tasks support mathematical learning?
Contributors


### 39	How can formative e-assessments improve students’ performance in end-of-module assessments?
Contributors


### 40	To what extent does the timing of assessment affect student learning?
Contributors


### 41	Is there a difference between performance in traditional and flipped or blended learning environments using e-assessment?
Contributors


### 42	When writing multiple choice questions, is student learning better enhanced using distractors based on common errors, or randomly-generated distractors?
Contributors


### 43	What principles should inform the design of mathematics e-assessment tasks?
Contributors


### 44	How can e-assessments be designed to impact students' example spaces?
Contributors


### 45	To what extent can current e-assessments meaningfully judge student responses to example generation tasks?
Contributors


### 46	How does the use of e-assessment impact students’ example generation strategies and success, relative to the same tasks on paper or orally?
Contributors


### 47	How can we automate the assessment of mathematical work traditionally done using paper and pen?
Contributors


### 48	How can the suitability of e-assessment tools for summative assessment be improved by combining computer-marking and pen-marking?
Contributors


### 49	Are there differences in performance on mathematics problems presented and carried out on paper versus on the computer?
Contributors


### 50	What methods are available for student input of mathematics?
Contributors


### 51	When we design e-assessments often we think of a question we would ask on a 'normal' pen and paper exam and we 'translate' this in e-assessment. What are the implications, technicalities, affordances and drawbacks of this transfer?
Contributors


### 52	What effect does the use of random versions of a question (e.g. using parameterised values) have on the outcomes of e-assessment?
Contributors


### 53	How do students respond to automated feedback, and are there any differences with how they would respond to the same feedback from a teacher?
Contributors


### 54	How can e-assessment be used in group work when learning mathematics, and what effect does the group element have on individuals' learning?
Contributors


### 55	What advice and guidance (both practical and pedagogical) is available to mathematics lecturers about using e-assessment in their courses, and to what extent do they engage with it?
