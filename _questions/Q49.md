---
question_code: Q49 
question_num: 49 
question_text: Are there differences in performance on mathematics problems presented and carried out on paper versus on the computer? 

question_code_meeting1: B1 
question_code_conf: TD8 

contributors: 
- timlowe
- prowlett

---

Does the different submission medium of computer based questions and paper based questions affect students performance?

## What motivates this question?

Previous research has found systematic differences in performance between paper-based and online versions of the same test (Backes and Cowan, 2019).

There are two main differences between computer-based and paper-based mathematical assessments which could affect the final mark awarded to a student.

A: The medium used to submit answers. Does the need to enter answers into a computer affect the correctness of that answer? Possible issues that could affect this include
1. The need to enter the answer using some computer syntax or equation editor, which might lead to errors
2. Whether students carry out working on paper before entering their answer into a computer or not. Any lack of working may result in slips due to mental-only processing of the problem, whereas students submitting on paper may be more inclinded to produce more complete paper workings whilst solving a problem.

B: The method used for marking. Computer marking may be more consistent than human marking, since human markers may miss small errors in one submission when marking a complete set of submissions, yet human markers are more able to use their intuition to (rightly or wrongly) award marks where the believe they know what a student meant to say, even if they did not fully express it. Automated marking is also in principle objective, fair and reliable, whereas doubts have been expressed about the consistency and quality of human markers (Foster, 2007). However, an e-assessment system should be monitored because systematic marking errors may take place (Ferrão, 2010).

## What might an answer look like?

*TODO: Description of research that may be carried out to address this question - ideally including planned work by WG members!*

Could compare the performace of balanced groups of students on online/paper tests. Ensuring the equity of the groups would be hard?
Question involving the production of diagrams might be an interesting example to use.
Do either system favour students with different accessibility needs?

## Related questions

* The issue of Y is related to [Q0: Why is XXX?](Q0)

* The issue of equity between paper and computer assignments is related to [Q17: How can we emulate human marking of students' mathematical working such as follow-on marking and partially correct marking](Q17)

* This question is related to [Q1](Q1): Do the errors students make in e-assessments differ from those they make in paper-based assessments?
* This question is related to [Q27: How can we assess open-ended mathematical tasks using e-assessment? ](Q27) and [Q47: How can we automate the assessment of mathematical work traditionally done using paper and pen?](Q47).


## References

Backes, B., & Cowan, J. (2019). Is the pen mightier than the keyboard? The effect of online testing on measured student achievement. Economics of Education Review, 68, 89-103.

Butcher and Jordan: A comparsion of human and computer marking of short free-text student reponses: https://doi.org/10.1016/j.compedu.2010.02.012

Chase, C. (1968). The impact of some obvious variables on essay test scores. Journal of Educa-tional Measurement, 5:315–318.

Chase, C. (1986). Essay test scoring: Interaction of relevant variables. Journal of EducationalMeasurement, 23:33–41. 

Ferrão, M. (2010). E-assessment within the Bologna paradigm: evidence from Portugal. Assessment & Evaluation in Higher Education, 35(7), 819-830. https://doi.org/10.1080/02602930903060990

Foster, B. (2007). Using computer based assessment in first year mathematics and statistics degree courses at Newcastle University. MSOR Connections, 7(3), 41-45.

Markham, L. (1976). Influences of handwriting quality on teacher evaluation of written work. American Educational Research Journal, 13:277–283.

Mogey, N., & Cowey, J., & Paterson, J., & Purcell, M. (2012). Students’  choices  between  typing  andhandwriting in examinations.Active Learning in Higher Education, 13(2):117–128.

Mogey, N., & Paterson, J., & Burk, J., & Purcell, M. (2010). Typing compared with handwriting foressay examinations at university: letting the students choose. ALT-J, Research in LearningTechnology, 18(1):29–47. 


Russell, M., & Tao, W. (2004). Effects of handwriting and computer-print on composition scores: A follow-up to powers, fowles, farnum and ramsey. Practical Assessment,  Research and Evaluation, 9(1).
Soloff, S. (1973). Effect  of  non-content  factors  on  the  grading  of  essays. Graduate  Research  inEducation and Related Disciplines, 6:44–54.
