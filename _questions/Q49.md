---
question_code: Q49 
question_num: 49 
question_text: Are there differences in performance on mathematics problems presented and carried out on paper versus on the computer? 

question_code_meeting1: B1 
question_code_conf: TD8 

contributors: 
- timlowe
- prowlett

---

Does the different submission medium of computer based questions and paper based questions affect students performance?

## What motivates this question?

Previous research has found systematic differences in performance between paper-based and online versions of the same test (Backes and Cowan, 2019).


A: The medium used to submit answers. Does the need to enter answers into a computer affect the correctness of that answer? Possible issues that could affect this include
1. The need to enter the answer using some computer syntax or equation editor, which might lead to errors
2. Whether students carry out working on paper before entering their answer into a computer or not. Any lack of working may result in slips due to mental-only processing of the problem, whereas students submitting on paper may be more inclinded to produce more complete paper workings whilst solving a problem.

B: The method used for marking. Computer marking may be more consistent than human marking, since human markers may miss small errors in one submission when marking a complete set of submissions, yet human markers are more able to use their intuition to (rightly or wrongly) award marks where the believe they know what a student meant to say, even if they did not fully express it. Automated marking is also in principle objective, fair and reliable, whereas doubts have been expressed about the consistency and quality of human markers (Foster, 2007). However, an e-assessment system should be monitored because systematic marking errors may take place (Ferrão, 2010).


There are two main differences between computer-based and paper-based mathematical assessments which could affect the final mark awarded to a student. Two variables might influence marks on a writing test: (1) mode of presentation, which is the format of responses (handwritten or typed text) presented to scorers; and (2) mode of composition, which is the medium (paper or computer) used by respondents for producing responses (Russell, 2004).


Influences of handwriting quality on teacher evaluation of written work: 

The early studies by (James, 1927) and (Markham, 1976) found that quality of handwriting significantly influenced grades given to essays. 
Many researchers have consistently studied the association of poor handwriting with lower marks from graders (e.g., (Chase, 1986), (Markham, 1976), (Chase, 1968), and (Soloff, 1973).)

## What might an answer look like?

An experiment could investigate the effect of the medium on the students' answers. 
For instance, students could be divided into "writing" and "typing online" groups and asked to solve the same problems. This would enable comparison of the performance between the two groups, both in terms of the correctness of the answers, and the features (such as length and quality of argument) of the answers. 



## Related questions

* The issue of Y is related to [Q0: Why is XXX?](Q0)

* The issue of equity between paper and computer assignments is related to [Q17: How can we emulate human marking of students' mathematical working such as follow-on marking and partially correct marking](Q17)

* This question is related to [Q1](Q1): Do the errors students make in e-assessments differ from those they make in paper-based assessments?
* This question is related to [Q27: How can we assess open-ended mathematical tasks using e-assessment? ](Q27) and [Q47: How can we automate the assessment of mathematical work traditionally done using paper and pen?](Q47).


## References

Backes, B., & Cowan, J. (2019). Is the pen mightier than the keyboard? The effect of online testing on measured student achievement. Economics of Education Review, 68, 89-103.

Butcher and Jordan: A comparsion of human and computer marking of short free-text student reponses: https://doi.org/10.1016/j.compedu.2010.02.012

Chase, C. (1968). The impact of some obvious variables on essay test scores. Journal of Educational Measurement, 5:315–318.

Chase, C. (1986). Essay test scoring: Interaction of relevant variables. Journal of Educational Measurement, 23:33–41. 

Ferrão, M. (2010). E-assessment within the Bologna paradigm: evidence from Portugal. Assessment & Evaluation in Higher Education, 35(7), 819-830. https://doi.org/10.1080/02602930903060990

Foster, B. (2007). Using computer based assessment in first year mathematics and statistics degree courses at Newcastle University. MSOR Connections, 7(3), 41-45.

James, A. (1927). The effect of handwriting on grading.English Journal, 16:180–205.

Markham, L. (1976). Influences of handwriting quality on teacher evaluation of written work. American Educational Research Journal, 13:277–283.

Russell, M., & Tao, W. (2004). Effects of handwriting and computer-print on composition scores: A follow-up to powers, fowles, farnum and ramsey. Practical Assessment,  Research and Evaluation, 9(1).

Soloff, S. (1973). Effect of non-content factors on the grading of essays. Graduate Research in Education and Related Disciplines, 6:44–54.
