---
question_code: Q18 
question_num: 18 
question_text: How can lecturers and task designers be informed about the interaction of university mathematics students in formative computer-based tasks in real practice to help them act upon these findings in an effective way? 

question_code_meeting1: B3 
question_code_conf: I5 

contributors: 
- timlowe
- Andre Heck

---

## What motivates this question?

In formative computer-aided assessment, feedback is mostly directed toward students. It gives them information about their performance in a task and is meant to help them improve their competencies. But how can lecturers know about the use of the assessment by their students and their progression in learning? But even when such information is available to the lecturer, how can(s)he support student in their learning path, during and after the formative assessment?   Similar questions hold for task designers: how can they know how students really interact with the  formative assessment tools and which intermediate steps they made toward a solution? How can they use this knowledge to improve their task design?   Gaining knowledge about what students really think and do while interacting with a digital learning environment is difficult. Learning analytics seems to focus on tracing students’ interactions with the learning tools. Even though learning analytics provides an impression of the progress of individual students and the class as a whole, it is difficult to relate the traces left by students in their work to their thinking processes and the possible interactions with the lecturer or fellow students during the tasks. Also it is not easy to act adequately and in time upon findings.  


## What might an answer look like?

The methods in italics below were suggested in the workshops as part of the methodology for developing this research agenda. However the below methods perhaps relate better to the theme student behaviour and specifically [Q35](Q35) and [Q53](Q53). Surely the method here should be about identifying mechanisms for informing lecturers and designers? For example STACK offers variant-level data on student performance and inputs, but is not very accessible.

_These questions could be addressed through further development of learning analytics methods and through observational studies (e.g. thinking-aloud studies with students while there are taking part in formative assessments). Comparison studies of HE practice regarding these aspects of formative computer-aided assessment seem informative as well.

_Research to answer this question might involve the use of eye-tracking software, or asking students to "speak their thoughts" whilst working 
through an e-assessment exercise.

_It might also involve social networking methods (Alcock et al. 2020), as well as observation protocols and interviews (Dorko 2020).


_
## Related questions

In section Errors and Feedback > Student errors along with:

- [Q20: What might a "hierarchy of needs" look like for mathematics lecturers who are transitioning to increased use of e-assessments?](Q20)
- [Q55: What advice and guidance (both practical and pedagogical) is available to mathematics lecturers about using e-assessment in their courses, and to what extent do they engage with it?](Q55)
- [Q35: How do mathematics students engage with questions and feedback provided by an e-assessment system?](Q35) 
- [Q53: How do students engage with automated feedback, and are there any differences with how they would respond to the same feedback from a teacher?](Q53)

## References

Alcock, L., Hernandez-Martinez, P., Patel, A. G., & Sirl, D. (2020). Study habits and attain-ment in undergraduate mathematics: A social network analysis. Journal for Research in Mathematics Education,51(1), 26–49

Dorko, A. (2020). Red X’s and Green Checks: A Model of How Students Engage with OnlineHomework.International Journal of Research in Undergraduate Mathematics Education,6(3), 446–474. 
