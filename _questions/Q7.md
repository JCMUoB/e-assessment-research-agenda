---
question_code: Q7 
question_num: 7 
question_text: What difficulties appear when designing e-assessment tasks that give constructive feedback to students? 

question_code_meeting1: A14
question_code_conf: EF7 

contributors: 
- ianjones
- niclaslarson
- prowlett

---
*TODO: Expanding on the question in 2-3 sentences*

## What motivates this question?

To make a computer to give constructive feedback to answers, and maybe to solutions, is a complex process. How can that be handled, and what difficulties will appear? This model might be relevant at the University of Adger already in the autumn 2020, in the engineering education at campus Grimstad.

Stepping back, we should consider intrinsic or implicit feedback vs external or explicit feedback. For example, consider the difference between a message popping up to say an answer is right or wrong, vs a GeoGebra construction where a student 'discovers' an inherent flaw in their work. 


## What might an answer look like?

Drawing on school-based research in which feedback is inherent [example](https://doi.org/10.5951/jresematheduc.43.1.0002) and game-based learning research [example](https://doi.org/10.3389/feduc.2019.00118) may help with this. Contributor Ian Jones will be supervising a project looking at the primary arithmetic app [Stick and Split](https://www.stickandsplit.com) which, not HE or assessment (sorry!), but will involve considering the role of implicit feedback.

It might be worth investigating why some in the literature report much positivity about instant, personalised feedback and others who report dissatisfaction with the feedback generated by e-assessment systems. For example, Broughton et al. (2017) report lecturers feeling the feedback given by their system was “not to the standard that [they] desired to give to their students”, including one lecturer who doubted the system was helping her weaker students due to “reservations towards the quality and helpfulness of the feedback” (see also e.g. Delius, 2004; Schofield & Ashton, 2005). Is the design of the e-assessment system that is used relevant to this, or is this problem more inherent in automated feedback systems? Perhaps a survey of e-assessment users would identify features of systems that are not universal but are relevant to the effectiveness of feedback.

## Related questions

* The issue of feedback is related to [Q2: What are the approaches to detecting and feeding back on students' errors?](Q2)
* Concerns about automated feedback effectiveness are discussed in [Q3](Q3).

## References

Broughton, S.J., Hernandez-Martinez, P. & Robinson, C.L. (2017). The effectiveness of computer-aided assessment for purposes of a mathematical sciences lecturer. In M. Ramirez-Montoya (Ed.), Handbook of Research on Driving STEM Learning with Educational Technologies (pp. 427-443). Hershey, PA: IGI Global.

Delius, G.W. (2004). Conservative Approach to Computerised Marking of Mathematics Assignments. MSOR Connections, 4(3), 42-47. 

Jay, T., Habgood, J., Mees, M., & Howard-Jones, P. (2019). Game-based training to promote arithmetic fluency. Frontiers in Education, 4, 118. https://doi.org/10.3389/feduc.2019.00118

Jones, I., & Pratt, D. (2012). A substituting meaning for the equals sign in arithmetic notating tasks. Journal for Research in Mathematics Education, 43(1), 2–33. https://doi.org/10.5951/jresematheduc.43.1.0002

Schofield, D. & Ashton, H. (2005). Effective reporting for online assessment --- shedding light on student behaviour. Maths-CAA Series, February 2005. Retrieved from http://icse.xyz/mathstore/node/61.html
